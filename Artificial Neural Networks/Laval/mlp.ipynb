{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #1 : Classification d'incidents avec un réseau *feedforward* et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la classification des descriptions d’accidents du premier travail. Le corpus de textes contient 3 partitions : \n",
    "-\tun fichier d’entraînement -  data/incidents_train.json\n",
    "-\tun fichier de validation -  data/incidents_dev.json\n",
    "-\tun fichier de test - data/incidents_test.json\n",
    "\n",
    "Entraînez un modèle de réseau de neurones de type feedforward multicouche (MLP) avec plongements de mots pour déterminer le type d’un incident à partir de sa description. \n",
    "\n",
    "Voici les consignes pour cette tâche : \n",
    "\n",
    "-\tNom du notebook : mlp.ipynb\n",
    "-\tTokenisation : Utilisation de Spacy. \n",
    "-\tPlongements de mots : Ceux de Spacy. \n",
    "-\tNormalisation : Aucune normalisation. \n",
    "-\tAgrégation des plongements de mots : Comparer les approches max, average et min pooling. \n",
    "-\tStructure du réseau : 1 seule couche cachée dont vous choisirez la taille (à expliquer). \n",
    "-\tPrésentez clairement vos résultats et faites-en l’analyse. En cas de doute, inspirez-vous de ce qui a été fait dans le travail pratique #1. \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Charger le modèle de langue de spaCy \n",
    "# !python -m spacy download fr_core_news_md pour télécharger\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "# Définir le Dataset\n",
    "class IncidentDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        label = int(self.data[idx]['label'])\n",
    "        # Tokenisation et plongements de mots avec spaCy\n",
    "        doc = nlp(text)\n",
    "        embeddings = [token.vector for token in doc]\n",
    "        return torch.tensor(embeddings), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taha\\Coding\\Academic-Projects\\Artificial Neural Networks\\Laval\n"
     ]
    }
   ],
   "source": [
    "# print current directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Academic-Projects\\\\Artificial Neural Networks\\\\Laval\\\\data\\\\incidents_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\taha\\Coding\\Academic-Projects\\Artificial Neural Networks\\Laval\\mlp.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Charger les données\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_data \u001b[39m=\u001b[39m IncidentDataset(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAcademic-Projects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mArtificial Neural Networks\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mLaval\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mincidents_train.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dev_data \u001b[39m=\u001b[39m IncidentDataset(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAcademic-Projects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mArtificial Neural Networks\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLaval\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mincidents_dev.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_data \u001b[39m=\u001b[39m IncidentDataset(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAcademic-Projects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mArtificial Neural Networks\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLaval\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mincidents_test.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\taha\\Coding\\Academic-Projects\\Artificial Neural Networks\\Laval\\mlp.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, file):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/taha/Coding/Academic-Projects/Artificial%20Neural%20Networks/Laval/mlp.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\taha\\Coding\\ocr_test\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\Academic-Projects\\\\Artificial Neural Networks\\\\Laval\\\\data\\\\incidents_train.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger les données\n",
    "train_data = IncidentDataset(r'Academic-Projects\\Artificial Neural Networks\\Laval\\data\\incidents_train.json')\n",
    "dev_data = IncidentDataset(r'Academic-Projects\\Artificial Neural Networks\\Laval\\data\\incidents_dev.json')\n",
    "test_data = IncidentDataset(r'Academic-Projects\\Artificial Neural Networks\\Laval\\data\\incidents_test.json')\n",
    "\n",
    "# Définir le DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.max(x, dim=0)[0]  # max pooling\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialiser le MLP\n",
    "model = MLP(300, 100, 6)  # 300 = dimension des plongements de mots de spaCy, 100 = taille de la couche cachée, 6 = nombre de classes\n",
    "\n",
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(10):  # 10 = nombre d'époques\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
