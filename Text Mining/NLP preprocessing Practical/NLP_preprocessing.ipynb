{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d99abf",
   "metadata": {},
   "source": [
    "### Practical example : Must Known Techniques for text preprocessing in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f50f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77992058",
   "metadata": {},
   "source": [
    "Now we will load data and perform some basic preprocessing to see the data.\n",
    "\n",
    " So, we are going to use Email spam data to demonstrate each technique and clean the data. The dataset contains 5730 unique email and a label column indicating mail is span or Ham which is the target variable on which based on the content we can classify the mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c80b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('emails.csv', usecols=['spam','text'])\n",
    "data.rename(columns={'spam':'class'},inplace=True)\n",
    "data['label'] = np.where(data['class']==1,'spam','ham')\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "263364d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>Subject: re : research and development charges...</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>Subject: re : receipts from visit  jim ,  than...</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>Subject: re : enron case study update  wow ! a...</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>Subject: re : interest  david ,  please , call...</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>Subject: news : aurora 5 . 2 update  aurora ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5697 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class label\n",
       "0     Subject: naturally irresistible your corporate...     1   ham\n",
       "1     Subject: the stock trading gunslinger  fanny i...     1   ham\n",
       "2     Subject: unbelievable new homes made easy  im ...     1   ham\n",
       "3     Subject: 4 color printing special  request add...     1   ham\n",
       "4     Subject: do not have money , get software cds ...     1   ham\n",
       "...                                                 ...   ...   ...\n",
       "5725  Subject: re : research and development charges...     0   ham\n",
       "5726  Subject: re : receipts from visit  jim ,  than...     0   ham\n",
       "5727  Subject: re : enron case study update  wow ! a...     0   ham\n",
       "5728  Subject: re : interest  david ,  please , call...     0   ham\n",
       "5729  Subject: news : aurora 5 . 2 update  aurora ve...     0   ham\n",
       "\n",
       "[5697 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934091e",
   "metadata": {},
   "source": [
    "Now we will start with the techniques for text preprocessing and clean the data which is ready to build a machine learning model. let us see the first mail and when we will apply the text cleaning technique we will observe the changes to the first mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45145c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e3dee",
   "metadata": {},
   "source": [
    "We can observe lots of noise at first mail like extra spaces, many hyphen marks « - » , different cases, and many more. let’s get started with studying different techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590eaab",
   "metadata": {},
   "source": [
    "1) Expand Contractions\n",
    "\n",
    "Contraction is the shortened form of a word like don’t stands for do not, aren’t stands for are not. Like this, we need to expand this contraction in the text data for better analysis. you can easily get the dictionary of contractions on google or create your own and use the re module to map the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e183b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "# Expanding Contractions in the reviews\n",
    "data['text']=data['text'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff6ece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34d042",
   "metadata": {},
   "source": [
    "2) Lower Case\n",
    "\n",
    "If the text is in the same case, it is easy for a machine to interpret the words because the lower case and upper case are treated differently by the machine. for example, words like Ball and ball are treated differently by machine. So, we need to make the text in the same case and the most preferred case is a lower case to avoid such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "941d2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.lower()#Use .str.lower instead of just .lower\n",
    "#the code with lambda function \n",
    "#data['text'] = data['text'].apply(lambda x:x.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d96f6",
   "metadata": {},
   "source": [
    "we have used a sub-method that takes 3 main parameters, the first is a pattern to search, the second is by which we have to replace, and the third is string or text which we have to change. so we have passed all the punctuation and finds if anyone present then replaces with an empty string. Now if you look at the first mail it will look something like this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d116daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"subject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , stylish statlonery and outstanding website  will make the task much easier .  we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader : it isguite ciear that  without good products , effective business organization and practicable aim it  will be hotat nowadays market ; but we do promise that your marketing efforts  will become much more effective . here is the list of clear  benefits : creativeness : hand - made , original logos , specially done  to reflect your distinctive company image . convenience : logo and stationery  are provided in all formats ; easy - to - use content management system letsyou  change your website content and even its structure . promptness : you  will see logo drafts within three business days . affordability : your  marketing break - through shouldn ' t make gaps in your budget . 100 % satisfaction  guaranteed : we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration . have a look at our  portfolio _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a4c78",
   "metadata": {},
   "source": [
    "-->You can observe the complete text in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f14f69",
   "metadata": {},
   "source": [
    "3) Remove punctuations\n",
    "\n",
    "\n",
    "One of the other text processing techniques is removing punctuations. there are total 32 main punctuations that need to be taken care of. we can directly use the string module with a regular expression to replace any punctuation in text with an empty string. 32 punctuations which string module provide us is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08553680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n",
    "#'!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "156a9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "data['text'] = data['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29938a1",
   "metadata": {},
   "source": [
    "we have used a sub-method that takes 3 main parameters, the first is a pattern to search, the second is by which we have to replace, and the third is string or text which we have to change. so we have passed all the punctuation and finds if anyone present then replaces with an empty string. Now if you look at the first mail it will look something like this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecb8c466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject naturally irresistible your corporate identity  lt is really hard to recollect a company  the  market is full of suqgestions and the information isoverwhelminq  but a good  catchy logo  stylish statlonery and outstanding website  will make the task much easier   we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader  it isguite ciear that  without good products  effective business organization and practicable aim it  will be hotat nowadays market  but we do promise that your marketing efforts  will become much more effective  here is the list of clear  benefits  creativeness  hand  made  original logos  specially done  to reflect your distinctive company image  convenience  logo and stationery  are provided in all formats  easy  to  use content management system letsyou  change your website content and even its structure  promptness  you  will see logo drafts within three business days  affordability  your  marketing break  through shouldn  t make gaps in your budget  100  satisfaction  guaranteed  we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration  have a look at our  portfolio                                                     not interested                                                       '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d604ba",
   "metadata": {},
   "source": [
    "4) Remove words containing digits and digits \n",
    "\n",
    "Sometimes it happens that words and digits combine are written in the text which creates a problem for machines to understand. hence, We need to remove the words and digits which are combined like game57 or game5ts7. This type of word is difficult to process so better to remove them or replace them with an empty string. we use regular expressions for this. \n",
    "\n",
    "The first mail is not having digits but other mails in the dataset contain this problem like mail 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e2846c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject 4 color printing special  request additional information now  click here  click here for a printable version of our order form  pdf format   phone   626  338  8090 fax   626  338  8102 e  mail  ramsey  goldengraphix  com  request additional information now  click here  click here for a printable version of our order form  pdf format   golden graphix  printing 5110 azusa canyon rd  irwindale  ca 91706 this e  mail message is an advertisement and  or solicitation  '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d083b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words and digits\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\b[0-9]+\\b\\s*', '',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9eef156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject color printing special  request additional information now  click here  click here for a printable version of our order form  pdf format   phone   fax   e  mail  ramsey  goldengraphix  com  request additional information now  click here  click here for a printable version of our order form  pdf format   golden graphix  printing azusa canyon rd  irwindale  ca this e  mail message is an advertisement and  or solicitation  '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now observe the changes in the mail.\n",
    "data['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e0c45",
   "metadata": {},
   "source": [
    "5) Remove Stopwords\n",
    "\n",
    "Stopwords are the most commonly occurring words in a text which do not provide any valuable information. stopwords like they, there, this, where, etc are some of the stopwords.\n",
    "\n",
    "NLTK library is a common library that is used to remove stopwords and include approximately 180 stopwords which it removes. If we want to add any new word to a set of words then it is easy using the add method.\n",
    "\n",
    "In our example, we want to remove the subject words from every mail so we will add them to stopwords and HTTP to remove web links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "388aab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "#here we have implemented a custom function that will split each word from the text and check whether it is a stopword or not.\n",
    "#If not then pass as it is in string and if stopword then removes it.\n",
    "data['text'] = data['text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec7090",
   "metadata": {},
   "source": [
    "Now the email text will be smaller because all stopwords will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea399784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'color printing special request additional information click click printable version order form pdf format phone fax e mail ramsey goldengraphix com request additional information click click printable version order form pdf format golden graphix printing azusa canyon rd irwindale ca e mail message advertisement solicitation'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now observe the changes in the mail.\n",
    "data['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e1eba",
   "metadata": {},
   "source": [
    "6) Stemming and Lemmatization\n",
    "\n",
    "Stemming is a process to reduce the word to its root stem for example run, running, runs, runed derived from the same word as run. basically stemming do is remove the prefix or suffix from word like ing, s, es, etc. NLTK library is used to stem the words. The stemming technique is not used for production purposes because it is not so efficient technique and most of the time it stems the unwanted words. So, to solve the problem another technique came into the market as Lemmatization. there are various types of stemming algorithms like porter stemmer, snowball stemmer. Porter stemmer is widely used present in the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38c3af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: stem_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ffdadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'color print special request addit inform click click printabl version order form pdf format phone fax e mail ramsey goldengraphix com request addit inform click click printabl version order form pdf format golden graphix print azusa canyon rd irwindal ca e mail messag advertis solicit'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3323a5c",
   "metadata": {},
   "source": [
    "Lemmatization is similar to stemming, used to stem the words into root word but differs in working. Actually, Lemmatization is a systematic way to reduce the words into their lemma by matching them with a language dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60826069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tahae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b90784a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19788/2390015160.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review'"
     ]
    }
   ],
   "source": [
    "#Lemmatizing \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "data[\"review\"] = data[\"review\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bef0f7",
   "metadata": {},
   "source": [
    "-----> Now observe the difference between both the techniques, it has only stemmed those words which are really required as per Language dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11b88e",
   "metadata": {},
   "source": [
    "7) Remove Extra Spaces\n",
    "\n",
    "Most of the time text data contain extra spaces or while performing the above preprocessing techniques more than one space is left between the text so we need to control this problem. regular expression library performs well to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ba536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832cd97",
   "metadata": {},
   "source": [
    "These are the most important text preprocessing techniques that are mostly used while dealing with NLP problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787a839",
   "metadata": {},
   "source": [
    "## Bag of Words using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75836cc0",
   "metadata": {},
   "source": [
    "sklearns count vectorizer method which does the following: \n",
    "<li>It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
    "<li>It counts the occurrence of each of those tokens.\n",
    "<li>The CountVectorizer method automatically converts all tokenized words to their lower case form . It does this using the lowercase parameter which is by default set to True.\n",
    "<li>It also ignores all punctuation.\n",
    "<li>The third parameter to take note of is the stop_words parameter. By setting this parameter value to english, CountVectorizer will automatically ignore all words(from our input text) that are found in the built in list of english stop words in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7de22",
   "metadata": {},
   "source": [
    "#### STEP 1 - Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0516bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus=data[\"text\"]\n",
    "count_vector = CountVectorizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2925d4",
   "metadata": {},
   "source": [
    " CountVectorizer() has certain parameters which take care of some preprocessing steps for us. They are:\n",
    "\n",
    "<li> lowercase = True\n",
    "\n",
    "The lowercase parameter has a default value of True which converts all of our text to its lower case form.\n",
    "\n",
    "<li> token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b\n",
    "\n",
    "The token_pattern parameter has a default regular expression value of (?u)\\\\b\\\\w\\\\w+\\\\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
    "\n",
    "<li> stop_words\n",
    "\n",
    "The stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192209b8",
   "metadata": {},
   "source": [
    "#### STEP 2 - Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words which have been categorized as features using the get_feature_names() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vector.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = count_vector.get_feature_names()\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ff984",
   "metadata": {},
   "source": [
    "#### STEP 3 - Create a matrix with the rows being each of the document, and the columns being each word. \n",
    "\n",
    "The corresponding (row, column) value is the frequency of occurrence of that word(in the column) in a particular document(in the row). You can do this using the transform() method and passing in the document data set as the argument. The transform() method returns a matrix of numpy integers, you can convert this to an array using toarray(). Call the array 'doc_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2678a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_array =  count_vector.fit_transform(corpus).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6e719",
   "metadata": {},
   "source": [
    "Now we have a clean representation of the documents in terms of the frequency distribution of the words in them. To make it easier to understand our next step is to convert this array into a dataframe and name the columns appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8172f",
   "metadata": {},
   "source": [
    "#### STEP 4 - Convert the array we obtained, loaded into 'doc_array', into a dataframe and set the column names to the word names(which you computed earlier using get_feature_names(). Call the dataframe 'frequency_matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array,columns=count_vector.get_feature_names())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18288415",
   "metadata": {},
   "source": [
    "##### Hyperparameters of BoW: –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ef027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating the new bag of words...')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 10,\n",
    "                            ngram_range=(1,2),\n",
    "                            #binary=True,\n",
    "                            min_df=0.1)#When building the vocabulary ignore terms that have a document  |      frequency strictly lower than the given threshold. This value is also|      called cut-off in the literature.\n",
    "\n",
    "vectorizer_features = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "vectorizer_features = vectorizer_features.toarray()\n",
    "print('Bag of words completed')\n",
    "new_frequency_matrix = pd.DataFrame(vectorizer_features,columns=vectorizer.get_feature_names())\n",
    "new_frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f71e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e830cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(vectorizer_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it\n",
    "# appears in the dataset\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe449a8b",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee7a12",
   "metadata": {},
   "source": [
    "Term Frequency measures how frequently a term occurs in a document.\n",
    "Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "\n",
    "  <li> TF(t) = (Number of times term t appears in a document)\n",
    "  \n",
    "    \n",
    "  \n",
    "Inverse Document Frequency measures how important a term is. While computing TF, all terms are considered equally important.\n",
    "\n",
    "    \n",
    "However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "  <li> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "      \n",
    "  \n",
    "  \n",
    "TF-IDF transform can be defined as the product of the term frequency and the inverse document frequency\n",
    "TfidfTransformer() takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a92517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tfidf ponderations')\n",
    "\n",
    "\n",
    "# Initialize the \"TfidfVectorizer\" object.\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vect.fit_transform(data['text'])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "vectorizer_features = X_tfidf.toarray()\n",
    "tfidf_frequency_matrix = pd.DataFrame(vectorizer_features,columns=tfidf_vect.get_feature_names())\n",
    "tfidf_frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176836f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tfidf_vect.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tfidf_vect.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:10]]))\n",
    "print(\"\\nFeatures with highest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd25272",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pylab as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=600).generate(\" \".join(data['text']))\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89f6ea",
   "metadata": {},
   "source": [
    "Word cloud will show the most frequent words in bigger size, whereas the less frequent words will be written with smaller size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6309b8b",
   "metadata": {},
   "source": [
    "Here are some notes regarding the arguments for WordCloud function:\n",
    "\n",
    "\n",
    "◼️ width/height: You can change the word cloud dimension to your preferred width and height with these.\n",
    "\n",
    "◼️ random_state: If you don’t this set this to a number of your choice, you are likely to get a slightly different word cloud every time you run the same script on the same input data. By setting this parameter, you ensure reproducibility of the exact same word cloud. You could play around with random numbers until you find the one that results in the word cloud you like.\n",
    "\n",
    "◼️ background_colour: ‘white’ and ‘black’ are common background colours. If you would like to explore more colours, this may come in handy. Please note that some colours may not work. Hope you will find something you fancy.\n",
    "\n",
    "◼️ colormap: With this argument, you can set up the colour theme that the words are displayed in. There are many beautiful Matplotlib colormaps to choose from. Some of my favourites are ‘rainbow’, ‘seismic’, ‘Pastel1’ and Pastel2’.\n",
    "\n",
    "◼️ collocations: Set this to False to ensure that the word cloud doesn’t appear as if it contains any duplicate words. Otherwise, you may see ‘web’, ‘scraping’ and ‘web scraping’ as a collocation in the word cloud, giving an impression that words have been duplicated.\n",
    "\n",
    "◼️ stopwords: Stopwords are common words which provide little to no value to the meaning of the text. ‘We’, ‘are’ and ‘the’ are examples of stopwords.\n",
    "\n",
    "\n",
    "There are other arguments that you can also customise. Check out the documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7a649",
   "metadata": {},
   "source": [
    "If you want to save the image, WordCloud provides a function to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ff149",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file('cloud.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
