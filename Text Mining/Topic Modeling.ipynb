{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling to segregate news report data to different topics using Gensim, NLTK, Spacy.\n",
    "\n",
    "Topic modelling as the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus.\n",
    "\n",
    "Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in <li>– “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, <li>and “farm”, “crops”, “wheat” for a topic – “Farming”. \n",
    "\n",
    "We have a dataset which consists of **News articles** and our task is to assign topics to those articles.\n",
    "    \n",
    "We will do a simple LSI and a LDA method to figure out the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task consist of :\n",
    "\n",
    "<lu> **1. Loading the data**\n",
    "    \n",
    "<lu> **2. Clean the Data**\n",
    "Transforming text into something an algorithm can digest it a complicated process. We cannot feed the data as it is, some preprocessing needs to be done. In this task we will be doing some preprocessing to convert our data in a form that we can feed our model with.\n",
    "    \n",
    "<li> **. Handling the Stop-words**\n",
    "    \n",
    "Text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words. We will remove these stopwords in this task.\n",
    "    \n",
    "<li> **. Lemmatization**\n",
    "    \n",
    "<lu> **3. TF-IDF Vectorization**\n",
    "    \n",
    "Apart from Count vectorizer an alternative to calculate word frequencies , and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "    \n",
    "   \n",
    "<lu> **4. Topic Modelling using Gensim's LDA**\n",
    "    \n",
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. \n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial. \n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. \n",
    "    \n",
    "<lu> **4. Topic modelling using LSA**\n",
    "    \n",
    "Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Dictionary with 'data' as key and each review as an element of list\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of articles\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with a column `document` containing all the articles\n",
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming text into something an algorithm can digest it a complicated process. We cannot feed the data \n",
    "as it is, some preprocessing needs to be done. \n",
    "In this task we will be doing some preprocessing to convert our data in a form that we can feed our model with.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df[:100]\n",
    "tokenized_doc = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(text):\n",
    "    #for token in text:\n",
    "    text =  ' '.join([w.lower() for w in text.split() if len(w)>3])\n",
    "    return text;\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(clean_doc)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the Stop-words and Punctuation-->Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "  \n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "custom = list(stopwords_set)+list(punctuation)\n",
    "\n",
    "def stopWordRemoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = ' '.join([word for word in text if word not in custom])\n",
    "    return text\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(stopWordRemoval)\n",
    "tokenized_doc\n",
    "\n",
    "# Initializing wordnet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemData(text):\n",
    "    text = word_tokenize(text)\n",
    "    newText = []\n",
    "    for word in text:\n",
    "        newText.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(newText)\n",
    "\n",
    "doc_clean = tokenized_doc.apply(lemData)\n",
    "doc_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of documents from the complaints column\n",
    "list_of_docs = doc_clean.tolist()\n",
    "\n",
    "# Implementing the function for all the complaints of list_of_docs\n",
    "doc_clean = [doc.split() for doc in list_of_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Some of the initialisations necessary for LDA are:\n",
    "* **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **word2id** is a mapping from words (strings) to word ids (integers). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **vocab** is a list of words (strings). It is used to determine the various words used in the text, as well as for debugging and topic printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "from gensim import corpora\n",
    "import string\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LDA model\n",
    "ldamodel = LdaModel(corpus=doc_term_matrix, num_topics=5,id2word=dictionary, random_state=20, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# printing the topics\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA on TFIDF vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LDA model\n",
    "ldamodel = LdaModel(corpus=corpus_tfidf, num_topics=5,id2word=dictionary, random_state=20, passes=30)\n",
    "\n",
    "# printing the topics\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is topic coherence?\n",
    "\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "#Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best K topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now define a function to compute the topic coherence for a given num_topics value and apply it to the num_topics values used in Latent Semantic Indexing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to find the optimal number of topics\n",
    "We need to build many LDA models with different values of the number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, step, start=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To DO \n",
    "#plot coherence distribution and choose best k\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=doc_clean, start=2, limit=10, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=10; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics. \n",
    "\n",
    "<li>If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that coherence score increases with the number of topics, with a decline between 3 to 4.\n",
    "\n",
    "Now, choosing the number of topics still depends on your requirement because topic around 5 have good coherence scores but may have repeated keywords in the topic. Topic coherence gives you a good picture so that you can take better decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis.gensim\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()#enable automatic D3 display of prepared model data in the IPython notebook.\n",
    "\n",
    "vis = gensimvis.prepare(ldamodel, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How to predict the topics for a new piece of text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that you have already built the topic model, you need to take the text through the same routine of transformations and before predicting the topic.\n",
    "\n",
    "For our case, the order of transformations is:\n",
    "\n",
    "sent_to_words() –> lemmatization() –> vectorizer.transform() –> best_lda_model.transform()\n",
    "\n",
    "You need to apply these transformations in the same order. So to simplify it, let’s combine these steps into a preprocess() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to predict topic for a given text document.\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Step 1: Clean with simple_preprocess\n",
    "    mytext_2 =  text.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    mytext_3=clean_doc(mytext_2)\n",
    "    mytext_4=stopWordRemoval(mytext_3)\n",
    "\n",
    "\n",
    "    # Step 2: Lemmatize\n",
    "    mytext_5 = lemData(mytext_4)\n",
    "    \n",
    "    # Step 3: Vectorize transform\n",
    "   # mytext_6 = dictionary.doc2bow(mytext_5)#tfidf.transform(mytext_3)\n",
    "\n",
    "    # Step 4: LDA Transform\n",
    "    #topic_probability_scores = ldamodel.transform(mytext_6)\n",
    "    #topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
    "    return mytext_5.split()#topic, topic_probability_scores\n",
    "\n",
    "# Predict the topic\n",
    "\n",
    "#topic, prob_scores = predict_topic(text = unseen_document)\n",
    "#print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  predicting new text \n",
    "new_text_corpus =  dictionary.doc2bow(preprocess(unseen_document))\n",
    "ldamodel[new_text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(ldamodel[new_text_corpus], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, ldamodel.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim LSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.lsimodel import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSi model\n",
    "lsimodel = LsiModel(corpus=doc_term_matrix, num_topics=5, id2word=dictionary)\n",
    "pprint(lsimodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopwords,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "fig.savefig('word_cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
